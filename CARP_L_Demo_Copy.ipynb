{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of CARP-L Demo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40907df287a3450ebcc9490e439913ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64681ee31ead423ba9283f61e29fb37e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb13da4867c14a1db3cfea1a7e078752",
              "IPY_MODEL_b253e7418ad4445ca5f976c20ff2a7cd",
              "IPY_MODEL_d6b355772c0f411bb8a6b0216019e5a6"
            ]
          }
        },
        "64681ee31ead423ba9283f61e29fb37e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb13da4867c14a1db3cfea1a7e078752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ffd9bd125f749db96d9f963b7ba745d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34f563e01a0843eca845f3e786a973c8"
          }
        },
        "b253e7418ad4445ca5f976c20ff2a7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8021dd159ebf467ca7a7a0b50e1081d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1912529,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1912529,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f7504f28b8d4c3e9d2efcbf721289b2"
          }
        },
        "d6b355772c0f411bb8a6b0216019e5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_407be51608744625a3a12e570bb33e96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.91M/1.91M [00:00&lt;00:00, 2.71MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c59cad571c6474abe9e71df7fd93a71"
          }
        },
        "5ffd9bd125f749db96d9f963b7ba745d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34f563e01a0843eca845f3e786a973c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8021dd159ebf467ca7a7a0b50e1081d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f7504f28b8d4c3e9d2efcbf721289b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "407be51608744625a3a12e570bb33e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c59cad571c6474abe9e71df7fd93a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GIM494/Learning-Design-Rules/blob/master/CARP_L_Demo_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gp44UlP7-6",
        "outputId": "ff7edfc6-f2d1-409d-a1c9-3c5cf2ecc6fe"
      },
      "source": [
        "!wget https://the-eye.eu/public/AI/CARP_L.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-12 20:44:20--  https://the-eye.eu/public/AI/CARP_L.pt\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2859997019 (2.7G) [application/octet-stream]\n",
            "Saving to: ‘CARP_L.pt’\n",
            "\n",
            "CARP_L.pt           100%[===================>]   2.66G  47.9MB/s    in 58s     \n",
            "\n",
            "2021-10-12 20:45:19 (46.8 MB/s) - ‘CARP_L.pt’ saved [2859997019/2859997019]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkrP7-vubY2C",
        "outputId": "e78fd5ae-4c70-4db3-ad8c-ed6b9a3c5391"
      },
      "source": [
        "!pip install torch \n",
        "!pip install transformers==4.6.0\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting transformers==4.6.0\n",
            "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.6.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8NWiH-AbFhF"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_FAQ18bSUl"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "754d5f88e92f4263a2cdda03d0aaeb57",
            "0f85c3ef79ba4fe2b8d0676ba16c7945",
            "23e639ebee1b4693a3c823baa995e698",
            "d4a0949056a04544a4044607408bab27",
            "6165749ce39f4de7a38dc3f58dff2335"
          ]
        },
        "id": "p3glmBmGbTQD",
        "outputId": "433fd6cb-15bc-4b68-ecd0-cfed5e167656"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "LATENT_DIM = 2048\n",
        "USE_CUDA = True\n",
        "USE_HALF = True\n",
        "config = transformers.RobertaConfig()\n",
        "\n",
        "extract_fns = {'EleutherAI/gpt-neo-1.3B' :\n",
        "                (lambda out : out['hidden_states'][-1]),\n",
        "                'EleutherAI/gpt-neo-2.7B' :\n",
        "                (lambda out : out['hidden_states'][-1]),\n",
        "                'roberta-large' : \n",
        "                (lambda out : out[0]),\n",
        "                'roberta-base' :\n",
        "                (lambda out : out[0]),\n",
        "                'microsoft/deberta-v2-xlarge' :\n",
        "                (lambda out : out[0])}\n",
        "\n",
        "d_models = {'EleutherAI/gpt-neo-1.3B' : 2048,\n",
        "            'EleutherAI/gpt-neo-2.7B' : 2560,\n",
        "            'roberta-large' : 1024,\n",
        "            'roberta-base' : 768,\n",
        "            'microsoft/deberta-v2-xlarge' : 1536}\n",
        "\n",
        "MODEL_PATH = \"roberta-large\"\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model = AutoModel.from_pretrained(MODEL_PATH)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "        self.d_model = d_models[MODEL_PATH]\n",
        "\n",
        "        # Add cls token to model and tokenizer\n",
        "        self.tokenizer.add_tokens(['[quote]'])\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def tok(self, string_batch):\n",
        "        return self.tokenizer(string_batch,\n",
        "                return_tensors = 'pt',\n",
        "                padding = True).to('cuda')\n",
        "    \n",
        "    def forward(self, x, mask = None, tokenize = False, mask_sum = True):\n",
        "        if tokenize:\n",
        "            x = self.tok(x)\n",
        "            mask = x['attention_mask']\n",
        "            x = x['input_ids']\n",
        "        \n",
        "        out = self.model(x, mask, output_hidden_states = True, return_dict = True)\n",
        "        \n",
        "        # out is a tuple of (model output, tuple)\n",
        "        # the second tuple is all layers\n",
        "        # in this second tuple, last elem is model output\n",
        "        # we take second last hidden -> third last layer\n",
        "        # size is always [batch, seq, 1536]\n",
        "        \n",
        "        hidden = out[0]\n",
        "        #layers = out[-1]\n",
        "        #hidden = layers[-2]\n",
        "        \n",
        "        # Mask out pad tokens embeddings\n",
        "        if mask_sum:\n",
        "            emb_mask = mask.unsqueeze(2).repeat(1, 1, self.d_model)\n",
        "            hidden = hidden * emb_mask\n",
        "\n",
        "        y = hidden.sum(1)\n",
        "        y = F.normalize(y)\n",
        "        \n",
        "        return y # Sum along sequence\n",
        "\n",
        "class ContrastiveModel(nn.Module):\n",
        "    def __init__(self, encA, encB):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encA = encA\n",
        "        self.encB = encB\n",
        "\n",
        "        self.projA = nn.Linear(self.encA.d_model, LATENT_DIM, bias = False)\n",
        "        self.projB = nn.Linear(self.encB.d_model, LATENT_DIM, bias = False)\n",
        "\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "        self.clamp_min = math.log(1/100)\n",
        "        self.clamp_max = math.log(100)\n",
        "\n",
        "    def clamp(self):\n",
        "        with torch.no_grad():\n",
        "            self.logit_scale.clamp(self.clamp_min, self.clamp_max)\n",
        "\n",
        "    def encodeX(self, x, masks = None):\n",
        "        x = self.encA(x, masks)\n",
        "        return self.projA(x)\n",
        "\n",
        "    def encodeY(self, y, masks = None):\n",
        "        y = self.encB(y, masks)\n",
        "        return self.projB(y)\n",
        "\n",
        "    # Calculate contrastive loss between embedding groups\n",
        "    # x, y are assumed encoding/embeddings here\n",
        "    def cLoss(self, x, y):\n",
        "        n = x.shape[0]\n",
        "        # normalize\n",
        "        x = F.normalize(x)\n",
        "        y = F.normalize(y)\n",
        "\n",
        "        logits = x @ y.T * self.logit_scale.exp()\n",
        "        labels = torch.arange(n, device ='cuda')\n",
        "\n",
        "        loss_i = F.cross_entropy(logits, labels)\n",
        "        loss_t = F.cross_entropy(logits.T, labels)\n",
        "        acc_i = (torch.argmax(logits, dim = 1) == labels).sum()\n",
        "        acc_t = (torch.argmax(logits, dim = 0) == labels).sum()\n",
        "\n",
        "        return (loss_i + loss_t) / 2, (acc_i + acc_t) / n / 2\n",
        "\n",
        "    def getLogits(self, x, y):\n",
        "        x = self.encodeX(*x)\n",
        "        y = self.encodeY(*y)\n",
        "\n",
        "        x = F.normalize(x)\n",
        "        y = F.normalize(y)\n",
        "\n",
        "        logits = x @ y.T * self.logit_scale.exp()\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.getLogits(x, y)\n",
        "\n",
        "model = ContrastiveModel(TextEncoder(), TextEncoder())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "754d5f88e92f4263a2cdda03d0aaeb57",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f85c3ef79ba4fe2b8d0676ba16c7945",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23e639ebee1b4693a3c823baa995e698",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4a0949056a04544a4044607408bab27",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6165749ce39f4de7a38dc3f58dff2335",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXzJVPhrfMID"
      },
      "source": [
        "model.load_state_dict(torch.load(\"/content/CARP_L.pt\"))\n",
        "if USE_HALF: model.half()\n",
        "if USE_CUDA: model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWilxq5NmQyw"
      },
      "source": [
        "N_CTX = 512\n",
        "def tok(string_batch):\n",
        "    for i, _ in enumerate(string_batch):\n",
        "        if len(string_batch[i]) > N_CTX:\n",
        "            string_batch[i] = string_batch[i][-N_CTX:]\n",
        "\n",
        "    return model.encA.tok(string_batch)\n",
        "\n",
        "def get_batch_tokens(dataset, inds):\n",
        "    batch = [dataset[ind] for ind in inds]\n",
        "    pass_batch = [pair[0] for pair in batch]\n",
        "    rev_batch = [pair[1] for pair in batch]\n",
        "\n",
        "    pass_tokens = tok(pass_batch)\n",
        "    rev_tokens = tok(rev_batch)\n",
        "    pass_masks = pass_tokens['attention_mask']\n",
        "    rev_masks = rev_tokens['attention_mask']\n",
        "    pass_tokens = pass_tokens['input_ids']\n",
        "    rev_tokens = rev_tokens['input_ids']\n",
        "\n",
        "    return pass_tokens, pass_masks, rev_tokens, rev_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebz-aThpt6l1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "40907df287a3450ebcc9490e439913ff",
            "64681ee31ead423ba9283f61e29fb37e",
            "bb13da4867c14a1db3cfea1a7e078752",
            "b253e7418ad4445ca5f976c20ff2a7cd",
            "d6b355772c0f411bb8a6b0216019e5a6",
            "5ffd9bd125f749db96d9f963b7ba745d",
            "34f563e01a0843eca845f3e786a973c8",
            "8021dd159ebf467ca7a7a0b50e1081d7",
            "5f7504f28b8d4c3e9d2efcbf721289b2",
            "407be51608744625a3a12e570bb33e96",
            "9c59cad571c6474abe9e71df7fd93a71",
            "af19b36451224fe9b6d4791ea18706be",
            "db91b69407ce4ca4b636e502e50c5f71",
            "9ab92621cc264f91a5a4c0b1b180bbc3",
            "da3bfdc0164e4f228d05057a3d617339"
          ]
        },
        "outputId": "3236288c-83b5-42c0-ffa7-47105d25fedb"
      },
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda'\n",
        "tokenizer_pegasus = PegasusTokenizer.from_pretrained(model_name)\n",
        "model_pegasus = PegasusForConditionalGeneration.from_pretrained(model_name).half().to(torch_device)\n",
        "#Paraphrases using peagasus. Used for softening.\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "  batch = tokenizer_pegasus([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model_pegasus.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer_pegasus.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40907df287a3450ebcc9490e439913ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af19b36451224fe9b6d4791ea18706be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db91b69407ce4ca4b636e502e50c5f71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ab92621cc264f91a5a4c0b1b180bbc3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da3bfdc0164e4f228d05057a3d617339",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2OYmA47uaU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4eb763-c384-460b-d322-db585258b529"
      },
      "source": [
        "get_response(\"Doesn't [quote] contradict [quote]?\", num_return_sequences=5, num_beams=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Doesn't it conflict with [quote]?\",\n",
              " \"Doesn't it conflict with thequote?\",\n",
              " \"Doesn't it conflict with the quote?\",\n",
              " \"Doesn't it conflict with what he said?\",\n",
              " \"Doesn't it conflict with [quote])?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2y7_Lmdongz"
      },
      "source": [
        "#Compute the logits of the passage against the reviews\n",
        "def get_passrev_logits(passages, reviews):\n",
        "    pass_tokens = tok(passages)\n",
        "    rev_tokens = tok(reviews)\n",
        "    pass_masks = pass_tokens['attention_mask']\n",
        "    rev_masks = rev_tokens['attention_mask']\n",
        "    pass_tokens = pass_tokens['input_ids']\n",
        "    rev_tokens = rev_tokens['input_ids']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model.getLogits([pass_tokens, pass_masks],\n",
        "                              [rev_tokens, rev_masks]).type(dtype=torch.float32)\n",
        "    return logits\n",
        "\n",
        "def report_logits(logits):\n",
        "    logits /= 2.7441\n",
        "    print((logits[0]).cpu().tolist())\n",
        "    conf = logits.softmax(1)\n",
        "\n",
        "    for i, row in enumerate(conf):\n",
        "        for j, col in enumerate(row):\n",
        "            print(str(i) + \"-\" + str(j) + \": \" + str(round(col.item(), 2)))\n",
        "\n",
        "def compute_softened_logits(passages, reviews1, reviews2, pairs=True):\n",
        "    \n",
        "    logits1 = torch.sum(get_passrev_logits(passages, reviews1), dim=-1).unsqueeze(0)/float(len(reviews1))\n",
        "    if pairs:\n",
        "      logits2 = torch.sum(get_passrev_logits(passages, reviews2), dim=-1).unsqueeze(0)/float(len(reviews2))\n",
        "\n",
        "      return torch.cat([logits1, logits2], dim=-1)\n",
        "    else:\n",
        "      return logits1\n",
        "#Lots of options to play with here that dictate how the paraphrases are generated.\n",
        "#Future work is needed\n",
        "def compute_logit(passages, reviews, soften=True,\n",
        "                        top_k=False, k = 3, \n",
        "                        ret = False, pairs=True):\n",
        "    #Softens the classifiers by using paraphrasing.\n",
        "    if soften:\n",
        "      if pairs:\n",
        "        review1_paraphrases = list(set(get_response(reviews[0], num_return_sequences=3, num_beams=3) + [reviews[0]]))\n",
        "        review2_paraphrases = list(set(get_response(reviews[1], num_return_sequences=3, num_beams=3) + [reviews[1]]))\n",
        "        print(review1_paraphrases)\n",
        "        print(review2_paraphrases)\n",
        "        \n",
        "        review1_contextual = list(map(lambda x: \"[quote] \" + x, review1_paraphrases)) \n",
        "        review2_contextual = list(map(lambda x: \"[quote] \" + x, review2_paraphrases)) \n",
        "\n",
        "        \n",
        "        softened_logits = compute_softened_logits(passages, review1_contextual + review1_paraphrases, review2_contextual + review2_paraphrases)\n",
        "        report_logits(softened_logits)\n",
        "        if ret: return softened_logits\n",
        "      else:\n",
        "        review_paraphrases = list(set(get_response(reviews, num_return_sequences=3, num_beams=3) + [reviews]))\n",
        "        #print(review_paraphrases)\n",
        "\n",
        "        review_contextual = list(map(lambda x: \"[quote] \" + x, review_paraphrases))\n",
        "        softened_logits = compute_softened_logits(passages, review_contextual + review_paraphrases, None, pairs=False)\n",
        "\n",
        "        #softened_logits = (softened_logits/2.7441)\n",
        "        print(softened_logits.squeeze().cpu().tolist())\n",
        "\n",
        "        if ret: return softened_logits\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dgxsT2pz-Kj"
      },
      "source": [
        "# Directly get logits for list of stories and critiques\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZA8xwprJgwF"
      },
      "source": [
        "stories = [\n",
        "           [\"The tiny lizard writhes in your grasp and claws at your fingers, its pink mouth gasping for breath. You squeeze harder and harder until your fist trembles with the effort. The lizard stops squirming.\"],\n",
        "           [\"All at once, the chest is lifted from you. Looking up, you see a man at the top of the cliff, pulling intently at the rope. “That is uncommonly good of you, I do say!” He chuckles unpleasantly.\"],\n",
        "           [\"You try to get close enough to the bulter, but he backs off. “That wouldn't be seemly, Miss.”\"],\n",
        "           [\"“No!” screams the Princess. The machine emits a dreadful grinding noise and goes through a series of complicated gyrations.\"],\n",
        "           [\"The man went to the store. He stole some cheese. His starving family was saved.\"],\n",
        "           [\"The man went to the store. He bought some cheese. He dropped it on the ground.\"]\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWPICoar0B3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ae4cf5-9271-4625-a602-4a5aedfb4f71"
      },
      "source": [
        "\n",
        "print(model.logit_scale)\n",
        "\n",
        "reviews = [\n",
        "  \"This is scary.\",\n",
        "  \"The behavior doesnt make sense.\",\n",
        "  \"The other characters wouldn't like this.\",\n",
        "  \"This doesn't make sense for the character to do.\",\n",
        "  \"This seems too nice.\",\n",
        "  \"This is too cheery.\",\n",
        "  \"This character doesn't fit.\"\n",
        "]\n",
        "\n",
        "\n",
        "#For every story, embed it and compute the cosine simarity of it against the embedded critique\n",
        "for story in stories:\n",
        "  print(story)\n",
        "\n",
        "  #Iterate over all tuples and determine which apply to this case\n",
        "  for pair in reviews:\n",
        "    print(pair)\n",
        "    compute_logit(story, pair, pairs=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor(2.7695, device='cuda:0', dtype=torch.float16, requires_grad=True)\n",
            "['The tiny lizard writhes in your grasp and claws at your fingers, its pink mouth gasping for breath. You squeeze harder and harder until your fist trembles with the effort. The lizard stops squirming.']\n",
            "This is scary.\n",
            "4.01806640625\n",
            "The behavior doesnt make sense.\n",
            "2.2373046875\n",
            "The other characters wouldn't like this.\n",
            "2.6298828125\n",
            "This doesn't make sense for the character to do.\n",
            "3.2080078125\n",
            "This seems too nice.\n",
            "1.3391265869140625\n",
            "This is too cheery.\n",
            "2.725341796875\n",
            "This character doesn't fit.\n",
            "0.7671382427215576\n",
            "['All at once, the chest is lifted from you. Looking up, you see a man at the top of the cliff, pulling intently at the rope. “That is uncommonly good of you, I do say!” He chuckles unpleasantly.']\n",
            "This is scary.\n",
            "2.179443359375\n",
            "The behavior doesnt make sense.\n",
            "1.8544921875\n",
            "The other characters wouldn't like this.\n",
            "3.53955078125\n",
            "This doesn't make sense for the character to do.\n",
            "3.457275390625\n",
            "This seems too nice.\n",
            "1.60491943359375\n",
            "This is too cheery.\n",
            "3.313232421875\n",
            "This character doesn't fit.\n",
            "2.4984130859375\n",
            "[\"You try to get close enough to the bulter, but he backs off. “That wouldn't be seemly, Miss.”\"]\n",
            "This is scary.\n",
            "1.0634765625\n",
            "The behavior doesnt make sense.\n",
            "3.2294921875\n",
            "The other characters wouldn't like this.\n",
            "3.08544921875\n",
            "This doesn't make sense for the character to do.\n",
            "2.09405517578125\n",
            "This seems too nice.\n",
            "0.5641732215881348\n",
            "This is too cheery.\n",
            "1.05596923828125\n",
            "This character doesn't fit.\n",
            "2.3848876953125\n",
            "['“No!” screams the Princess. The machine emits a dreadful grinding noise and goes through a series of complicated gyrations.']\n",
            "This is scary.\n",
            "4.01123046875\n",
            "The behavior doesnt make sense.\n",
            "1.65264892578125\n",
            "The other characters wouldn't like this.\n",
            "3.377685546875\n",
            "This doesn't make sense for the character to do.\n",
            "2.3792724609375\n",
            "This seems too nice.\n",
            "-1.1319503784179688\n",
            "This is too cheery.\n",
            "1.0338897705078125\n",
            "This character doesn't fit.\n",
            "1.034912109375\n",
            "['The man went to the store. He stole some cheese. His starving family was saved.']\n",
            "This is scary.\n",
            "1.6859130859375\n",
            "The behavior doesnt make sense.\n",
            "0.40478515625\n",
            "The other characters wouldn't like this.\n",
            "1.232666015625\n",
            "This doesn't make sense for the character to do.\n",
            "0.9444580078125\n",
            "This seems too nice.\n",
            "3.073974609375\n",
            "This is too cheery.\n",
            "3.685791015625\n",
            "This character doesn't fit.\n",
            "0.972564697265625\n",
            "['The man went to the store. He bought some cheese. He dropped it on the ground.']\n",
            "This is scary.\n",
            "1.88140869140625\n",
            "The behavior doesnt make sense.\n",
            "3.541259765625\n",
            "The other characters wouldn't like this.\n",
            "1.6278076171875\n",
            "This doesn't make sense for the character to do.\n",
            "2.44012451171875\n",
            "This seems too nice.\n",
            "1.52606201171875\n",
            "This is too cheery.\n",
            "1.06182861328125\n",
            "This character doesn't fit.\n",
            "0.731597900390625\n"
          ]
        }
      ]
    }
  ]
}